{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import statistics as stats\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has\n",
    "\n",
    "# Load single JSON files to inspect\n",
    "with open(\"../data/AK_Juneau_0.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Isolate \"results\" from json only\n",
    "json_results = json_data[\"data\"][\"results\"]\n",
    "\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(json_results)\n",
    "\n",
    "#Flatten nested dictionaries in data\n",
    "df = pd.json_normalize(df.to_dict(orient=\"records\"))\n",
    "\n",
    "#View summary of created dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to open JSON files and place them into normalized dataframe\n",
    "def load_json_file(file_path):\n",
    "    #open file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    #Isolate \"results\" from json only\n",
    "    json_results = json_data[\"data\"][\"results\"]\n",
    "\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(json_results)\n",
    "\n",
    "    #Flatten nested dictionaries in data\n",
    "    df = pd.json_normalize(df.to_dict(orient=\"records\"))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over all files and put them into a dataframe\n",
    "data_folder_path= \"../data\"\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in os.listdir(data_folder_path):\n",
    "    file_path = f\"{data_folder_path}/{filename}\"\n",
    "\n",
    "    #Skip if file is not JSON and is a not a file\n",
    "    if not filename.lower().endswith(\".json\") or not os.path.isfile(file_path):\n",
    "        print(f\"Skipping: {filename}...\")\n",
    "        continue\n",
    "\n",
    "    #Process file data into dataframe\n",
    "    print(f\"Processing: {filename}\")\n",
    "    temp_df = load_json_file(file_path)\n",
    "\n",
    "    #Add dataframe to list of all dataframes\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "#Merge all dataframes in list into single dataframe\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Take a quick look at your data (i.e. `.info()`, `.describe()`) - what do you see?\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price (target).  These rows should be dropped.\n",
    "- There are a lot of NA/None values.  Should these be dropped or replaced with something?\n",
    "    - You can drop rows or use various methods to fills NA's - use your best judgement for each column \n",
    "    - i.e. for some columns (like Garage), NA probably just means no Garage, so 0\n",
    "- Drop columns that aren't needed\n",
    "    - Don't keep the list price because it will be too close to the sale price. Assume we want to predict the price of houses not yet listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\t\n",
    "    'last_update_date',\n",
    "\t'permalink',\n",
    "\t'status',\n",
    "\t'open_houses',\n",
    "\t'branding',\n",
    "\t'list_price',\n",
    "\t'property_id',\n",
    "\t'photos',\n",
    "\t'community',\n",
    "\t'virtual_tours',\n",
    "\t'listing_id',\n",
    "\t'price_reduced_amount',\n",
    "\t'matterport',\n",
    "\t'primary_photo.href',\n",
    "\t'source.plan_id',\n",
    "\t'source.agents',\n",
    "\t'source.spec_id',\n",
    "\t'source.type',\n",
    "\t'lead_attributes.show_contact_an_agent',\n",
    "\t'flags.is_new_construction',\n",
    "\t'flags.is_for_rent',\n",
    "    'flags.is_subdivision',\n",
    "\t'flags.is_contingent',\n",
    "\t'flags.is_price_reduced',\n",
    "\t'flags.is_pending',\n",
    "\t'flags.is_foreclosure',\n",
    "\t'flags.is_plan',\n",
    "\t'flags.is_coming_soon',\n",
    "\t'flags.is_new_listing',\n",
    "\t'products.brand_name',\n",
    "\t'other_listings.rdc',\n",
    "\t'location.address.coordinate.lon',\n",
    "\t'location.address.coordinate.lat',\n",
    "\t'location.address.line',\n",
    "\t'location.street_view_url',\n",
    "\t'location.county.fips_code',\n",
    "\t'primary_photo',\n",
    "\t'source', \n",
    "\t'products',\n",
    "\t'location.address.coordinate',\n",
    "\t'other_listings',\n",
    "\t'community.advertisers',\n",
    "\t'community.description.name',\n",
    "\t'location.county',\n",
    "    'description.name',\n",
    "    'description.baths_1qtr'\n",
    "]\n",
    "\n",
    "combined_df = combined_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.baths_half'] = combined_df['description.baths_half'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.baths_3qtr'] = combined_df['description.baths_3qtr'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.baths_full'] = combined_df['description.baths_full'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.garage'] = combined_df['description.garage'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset=['description.sold_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These listings are missing lots of data\n",
    "combined_df.loc[combined_df['description.type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset=['description.type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing lots of data\n",
    "combined_df = combined_df[combined_df['description.type'] != 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.beds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numbers of beds to be the mode by each city for NaN values\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.beds'].at[index]):\n",
    "        city = combined_df['location.address.city'].at[index]\n",
    "        combined_df.at[index, 'description.beds'] = stats.mode(combined_df['description.beds'].loc[combined_df['location.address.city'] == city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "combined_df['description.type'].loc[combined_df['description.beds'].isna()].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took land only properties with NaN beds and filled 0\n",
    "\n",
    "combined_df['description.beds'].loc[combined_df['description.type'] == 'land'] = 0\n",
    "\n",
    "#same with year_built\n",
    "combined_df.loc[combined_df['description.type'] == 'land','description.year_built'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took 5 mobile home properties with NaN beds and filled as 0\n",
    "\n",
    "combined_df['description.beds'].loc[(combined_df['description.type'] == 'mobile') & (combined_df['description.beds'].isna())] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set land NaN values for stories to 0\n",
    "combined_df['description.stories'].loc[combined_df['description.type'] == 'land'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double decker mobile home check\n",
    "combined_df.loc[combined_df['description.type'] == 'mobile', 'description.stories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled the mobile homes missing stories with the values already in the DF, applied with a random probability equal to its occurence \n",
    "combined_df.loc[combined_df['description.type'] == 'mobile', 'description.stories'] = combined_df.loc[combined_df['description.type'] == 'mobile', 'description.stories'].apply(lambda x: np.random.choice([1, 2], p=[0.88, 0.12]) if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace condo_townhome_rowhome_coop with townhomes\n",
    "combined_df.loc[combined_df['description.type'] == 'condo_townhome_rowhome_coop', 'description.type'] = 'townhomes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[combined_df['description.type'] == 'townhome', 'description.type'] = 'townhomes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set numbers of stories to be the mode by city and property type\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.stories'].at[index]):\n",
    "        city = combined_df['location.address.city'].at[index]\n",
    "        county = combined_df['location.county.name'].at[index]\n",
    "        prop_type = combined_df['description.type'].at[index]\n",
    "        combined_df.at[index, 'description.stories'] = stats.mode(combined_df['description.stories'].loc[(combined_df['location.address.city'] == city) & (combined_df['description.type'] == prop_type)])\n",
    "        if np.isnan(combined_df['description.stories'].at[index]):\n",
    "            combined_df.at[index, 'description.stories'] = stats.mode(combined_df['description.stories'].loc[(combined_df['location.county.name'] == county) & (combined_df['description.type'] == prop_type)])\n",
    "            if np.isnan(combined_df['description.stories'].at[index]):\n",
    "                combined_df.at[index, 'description.stories'] = stats.mode(combined_df['description.stories'].loc[combined_df['description.type'] == prop_type])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking what types of properties are missing stories values\n",
    "combined_df['description.type'][combined_df['description.stories'].isna()].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['description.type','description.stories']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set year built to be the median by city and property type\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.year_built'].at[index]):\n",
    "        city = combined_df['location.address.city'].at[index]\n",
    "        county = combined_df['location.county.name'].at[index]\n",
    "        prop_type = combined_df['description.type'].at[index]\n",
    "        combined_df.at[index, 'description.year_built'] = stats.median(combined_df['description.year_built'].loc[(combined_df['location.address.city'] == city) & (combined_df['description.type'] == prop_type)])\n",
    "        if np.isnan(combined_df['description.year_built'].at[index]):\n",
    "            combined_df.at[index, 'description.year_built'] = stats.median(combined_df['description.year_built'].loc[(combined_df['location.county.name'] == county) & (combined_df['description.type'] == prop_type)])\n",
    "            if np.isnan(combined_df['description.year_built'].at[index]):\n",
    "                combined_df.at[index, 'description.year_built'] = stats.median(combined_df['description.year_built'].loc[combined_df['location.address.city'] == city])\n",
    "                if np.isnan(combined_df['description.year_built'].at[index]):\n",
    "                    combined_df.at[index, 'description.year_built'] = stats.median(combined_df['description.year_built'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for properties that arent just land and have no bathrooms entered, the bathrooms are set to the mode of the same property type\n",
    "\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if (combined_df.at[index, 'description.baths_full'] == 0) & (combined_df.at[index, 'description.baths_3qtr'] == 0) & (combined_df.at[index, 'description.baths_half'] == 0) & (combined_df.at[index, 'description.type'] != 'land'):\n",
    "        prop_type = combined_df['description.type'].at[index]\n",
    "        combined_df.at[index, 'description.baths_full'] = stats.mode(combined_df['description.baths_full'].loc[combined_df['description.type'] == prop_type])\n",
    "        combined_df.at[index, 'description.baths_3qtr'] = stats.mode(combined_df['description.baths_3qtr'].loc[combined_df['description.type'] == prop_type])\n",
    "        combined_df.at[index, 'description.baths_half'] = stats.mode(combined_df['description.baths_half'].loc[combined_df['description.type'] == prop_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.baths'].at[index]):\n",
    "        combined_df.at[index, 'description.baths'] = combined_df.at[index, 'description.baths_full'] + combined_df.at[index, 'description.baths_3qtr'] + combined_df.at[index, 'description.baths_half']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[(combined_df['description.baths_full'] == 0) & (combined_df['description.baths_3qtr'] == 0) & (combined_df['description.baths_half'] == 0) & (combined_df['description.type'] != 'land')]\n",
    "\n",
    "#after running above code in block above, it seems like condo as a type is missing quite a bit of info\n",
    "# ran this after to see what kind of info 'condo' had\n",
    "combined_df.loc[combined_df['description.type'] == 'condo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switched condo to condos to condense types and also fill missing data\n",
    "combined_df.loc[combined_df['description.type'] == 'condo', 'description.type'] = 'condos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used mean to fill in missing lot square footage according to property type and area\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.lot_sqft'].at[index]):\n",
    "        prop_type = combined_df['description.type'].at[index]\n",
    "        city = combined_df['location.address.city'].at[index]\n",
    "        county = combined_df['location.county.name'].at[index]\n",
    "        combined_df.at[index, 'description.lot_sqft'] = stats.mean(combined_df['description.lot_sqft'].loc[(combined_df['description.type'] == prop_type) & (combined_df['location.address.city'] == city)])\n",
    "        if np.isnan(combined_df['description.lot_sqft'].at[index]):\n",
    "            combined_df.at[index, 'description.lot_sqft'] = stats.mean(combined_df['description.lot_sqft'].loc[(combined_df['description.type'] == prop_type) & (combined_df['location.county.name'] == county)])\n",
    "            if np.isnan(combined_df['description.lot_sqft'].at[index]):\n",
    "                combined_df.at[index, 'description.lot_sqft'] = stats.mean(combined_df['description.lot_sqft'].loc[combined_df['description.type'] == prop_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[combined_df['description.lot_sqft'].isna(), 'description.type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used mean to fill in missing square footage according to property type and area\n",
    "for index, rows in combined_df.iterrows():\n",
    "    if np.isnan(combined_df['description.sqft'].at[index]) & (combined_df['description.type'].at[index] != 'land'):\n",
    "        prop_type = combined_df['description.type'].at[index]\n",
    "        city = combined_df['location.address.city'].at[index]\n",
    "        county = combined_df['location.county.name'].at[index]\n",
    "        combined_df.at[index, 'description.sqft'] = combined_df['description.sqft'].loc[(combined_df['description.type'] == prop_type) & (combined_df['location.address.city'] == city)].mean()\n",
    "        if np.isnan(combined_df['description.sqft'].at[index]) & (combined_df['description.type'].at[index] != 'land'):\n",
    "            combined_df.at[index, 'description.sqft'] = combined_df['description.sqft'].loc[(combined_df['description.type'] == prop_type) & (combined_df['location.county.name'] == county)].mean()\n",
    "            if np.isnan(combined_df['description.sqft'].at[index]) & (combined_df['description.type'].at[index] != 'land'):\n",
    "                combined_df.at[index, 'description.sqft'] = combined_df['description.sqft'].loc[combined_df['description.type'] == prop_type].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[combined_df['description.sqft'].isna(), 'description.type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['description.type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = [tag for sublist in combined_df['tags'].dropna() for tag in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tag_counts = Counter(all_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 1500\n",
    "valid_tags = {tag for tag, count in tag_counts.items() if count >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['filtered_tags'] = combined_df['tags'].apply(lambda tag_list: [tag for tag in tag_list if tag in valid_tags] if isinstance(tag_list, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_df = combined_df['filtered_tags'].explode().str.get_dummies().groupby(level=0).sum()\n",
    "ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_df = ohe_df.drop(columns=['garage_1_or_more', 'garage_2_or_more', 'single_story', 'two_or_more_stories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['tags']).join(ohe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_df2 = combined_df['description.type'].str.get_dummies()\n",
    "combined_df = combined_df.drop(columns='description.type').join(ohe_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- What we can do is use our training data to encode the mean sale price by city as a feature (a.k.a. Target Encoding)\n",
    "    - We can do this as long as we ONLY use the training data - we're using the available data to give us a 'starting guess' of the price for each city, without needing to encode city explicitly\n",
    "- If you replace cities or states with numerical values (like the mean price), make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Note that you *may* have cities in the test set that are not in the training set. You don't want these to be NA, so maybe you can fill them with the overall mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_df.drop(columns='description.sold_price')\n",
    "y = combined_df['description.sold_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.join(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, rows in X_train.iterrows():\n",
    "    city = X_train['location.address.city'].at[index]\n",
    "    state = X_train['location.address.state'].at[index]\n",
    "    X_train.at[index, 'location.address.city'] = X_train.loc[X_train['location.address.city'] == city, 'description.sold_price'].mean()\n",
    "    X_train.at[index, 'location.address.state'] = X_train.loc[X_train['location.address.state'] == state, 'description.sold_price'].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns='description.sold_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('../data/preprocessed/combined_df.csv')\n",
    "X_train.to_csv('../data/preprocessed/X_train.csv')\n",
    "X_test.to_csv('../data/preprocessed/X_test.csv')\n",
    "y_train.to_csv('../data/preprocessed/y_train.csv')\n",
    "y_test.to_csv('../data/preprocessed/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data - STRETCH\n",
    "\n",
    "> This doesn't need to be part of your Minimum Viable Product (MVP). We recommend you write a functional, basic pipeline first, then circle back and join new data if you have time\n",
    "\n",
    "> If you do this, try to write your downstream steps in a way it will still work on a dataframe with different features!\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/ Visualization\n",
    "\n",
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.    \n",
    "    - Consider transforming very skewed variables\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "    - You may have too many features to do this, in which case you can simply compute the most correlated feature-pairs and list them\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Finishing Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
