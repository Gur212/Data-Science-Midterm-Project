{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3381, 34)\n",
      "y_train shape: (3381, 1)\n",
      "X_test shape: (1450, 34)\n",
      "y_test shape: (1450, 1)\n"
     ]
    }
   ],
   "source": [
    "#Import preprocessed data\n",
    "import pandas as pd\n",
    "\n",
    "#Independant variable training data\n",
    "X_train = pd.read_csv(\"../data/preprocessed/X_train_scaled.csv\")\n",
    "X_train = X_train.drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "#Target training data\n",
    "y_train = pd.read_csv(\"../data/preprocessed/y_train.csv\")\n",
    "y_train = y_train.drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "#Independant variable test data\n",
    "X_test = pd.read_csv(\"../data/preprocessed/X_test_scaled.csv\")\n",
    "X_test = X_test.drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "#Target test data\n",
    "y_test = pd.read_csv(\"../data/preprocessed/y_test.csv\")\n",
    "y_test = y_test.drop(columns=[\"Unnamed: 0\"])\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to get all error scores at once\n",
    "def get_error_scores (y_train, y_train_pred, y_test, y_test_pred, error_type='All', num_results=10):\n",
    "    # Check performance on train and test set\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    if (error_type == 'All' or LOWER(error_type) == 'r2'):\n",
    "        #Using R2\n",
    "        r2_train = round(r2_score(y_train, y_train_pred),4)\n",
    "        r2_test = round(r2_score(y_test, y_test_pred),4)\n",
    "\n",
    "        print(f'R SQUARED\\n\\tTrain R²:\\t{r2_train}\\n\\tTest R²:\\t{r2_test}')\n",
    "\n",
    "    if (error_type == 'All' or LOWER(error_type) == 'mae'):\n",
    "        #Using Mean Average Error\n",
    "        MAE_train = round(mean_absolute_error(y_train, y_train_pred),2)\n",
    "        MAE_test = round(mean_absolute_error(y_test, y_test_pred),2)\n",
    "\n",
    "        print(f'MEAN AVERAGE ERROR\\n\\tTrain MAE:\\t{MAE_train}\\n\\tTest MAE:\\t{MAE_test}')\n",
    "\n",
    "    if (error_type == 'All' or LOWER(error_type) == 'rmse'):\n",
    "        #Using Root Mean Squared Error\n",
    "        RMSE_train = round(np.sqrt(mean_squared_error(y_train, y_train_pred)),2)\n",
    "        RMSE_test = round(np.sqrt(mean_squared_error(y_test, y_test_pred)),2)\n",
    "\n",
    "        print(f'ROOT MEAN SQUARED ERROR\\n\\tTrain RMSE:\\t{RMSE_train}\\n\\tTest RMSE:\\t{RMSE_test}\\n')\n",
    "\n",
    "    if (error_type == 'All'):\n",
    "        display_results_sample(y_test, y_test_pred, num_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to demonstrate of prediction\n",
    "def display_results_sample (y_test, y_test_prediction, num_results=10):\n",
    "    import random\n",
    "\n",
    "    print(f\"{num_results} Randomly selected results.\")\n",
    "\n",
    "    sum_percentage_error = 0\n",
    "\n",
    "    #Choose 10 rows to display\n",
    "    for i in range(num_results):\n",
    "        j = random.randint(0, len(y_test)-1)\n",
    "\n",
    "        demo_prediction = round(y_test_prediction[j][0])\n",
    "        demo_actual = round(y_test.iloc[j].item())\n",
    "        demo_difference = demo_prediction - demo_actual\n",
    "        demo_difference_percentage = round((demo_actual / demo_prediction - 1)*100,2)\n",
    "\n",
    "        sum_percentage_error += abs(demo_difference_percentage)\n",
    "\n",
    "        print(f\"Index: {j} \\t- \\tPrediction: ${demo_prediction:,} \\tActual: ${demo_actual:,} \\tDifference: {demo_difference:,}, {demo_difference_percentage}%\")\n",
    "\n",
    "    average_percentage_error = round(sum_percentage_error / num_results,2)\n",
    "    print(f\"\\t\\t\\t\\t\\t\\t\\t\\t\\tAverage % error = {average_percentage_error}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R SQUARED\n",
      "\tTrain R2:\t0.7374\n",
      "\tTest R2:\t0.7271\n",
      "MEAN AVERAGE ERROR\n",
      "\tTrain MAE:\t69045.11\n",
      "\tTest MAE:\t69106.02\n",
      "ROOT MEAN SQUARED ERROR\n",
      "\tTrain RMSE:\t94173.5\n",
      "\tTest RMSE:\t95747.86\n",
      "\n",
      "10 Randomly selected results.\n",
      "Index: 1356 \t- \tPrediction: $186,969 \tActual: $150,000 \tDifference: 36,969, -19.77%\n",
      "Index: 225 \t- \tPrediction: $567,266 \tActual: $762,000 \tDifference: -194,734, 34.33%\n",
      "Index: 1247 \t- \tPrediction: $446,066 \tActual: $280,000 \tDifference: 166,066, -37.23%\n",
      "Index: 1379 \t- \tPrediction: $563,924 \tActual: $368,000 \tDifference: 195,924, -34.74%\n",
      "Index: 1110 \t- \tPrediction: $51,367 \tActual: $100,000 \tDifference: -48,633, 94.68%\n",
      "Index: 84 \t- \tPrediction: $614,979 \tActual: $651,000 \tDifference: -36,021, 5.86%\n",
      "Index: 40 \t- \tPrediction: $164,046 \tActual: $153,000 \tDifference: 11,046, -6.73%\n",
      "Index: 396 \t- \tPrediction: $372,689 \tActual: $270,000 \tDifference: 102,689, -27.55%\n",
      "Index: 594 \t- \tPrediction: $299,558 \tActual: $245,000 \tDifference: 54,558, -18.21%\n",
      "Index: 820 \t- \tPrediction: $356,691 \tActual: $299,000 \tDifference: 57,691, -16.17%\n",
      "\t\t\t\t\t\t\t\t\tAverage % error = 29.53%\n"
     ]
    }
   ],
   "source": [
    "# Train our Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_train_pred = reg.predict(X_train)\n",
    "y_test_pred = reg.predict(X_test)\n",
    "\n",
    "get_error_scores (y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of polynomial features: 630\n",
      "R SQUARED\n",
      "\tTrain R2:\t0.8911\n",
      "\tTest R2:\t0.8373\n",
      "MEAN AVERAGE ERROR\n",
      "\tTrain MAE:\t45371.38\n",
      "\tTest MAE:\t54898.79\n",
      "ROOT MEAN SQUARED ERROR\n",
      "\tTrain RMSE:\t60651.3\n",
      "\tTest RMSE:\t73930.9\n",
      "\n",
      "10 Randomly selected results.\n",
      "Index: 638 \t- \tPrediction: $531,926 \tActual: $550,000 \tDifference: -18,074, 3.4%\n",
      "Index: 1389 \t- \tPrediction: $253,862 \tActual: $145,000 \tDifference: 108,862, -42.88%\n",
      "Index: 313 \t- \tPrediction: $487,708 \tActual: $529,900 \tDifference: -42,192, 8.65%\n",
      "Index: 1382 \t- \tPrediction: $221,235 \tActual: $315,000 \tDifference: -93,765, 42.38%\n",
      "Index: 972 \t- \tPrediction: $39,779 \tActual: $74,250 \tDifference: -34,471, 86.66%\n",
      "Index: 283 \t- \tPrediction: $486,318 \tActual: $450,000 \tDifference: 36,318, -7.47%\n",
      "Index: 73 \t- \tPrediction: $510,943 \tActual: $785,000 \tDifference: -274,057, 53.64%\n",
      "Index: 834 \t- \tPrediction: $313,310 \tActual: $200,000 \tDifference: 113,310, -36.17%\n",
      "Index: 1108 \t- \tPrediction: $256,544 \tActual: $288,000 \tDifference: -31,456, 12.26%\n",
      "Index: 485 \t- \tPrediction: $171,267 \tActual: $106,000 \tDifference: 65,267, -38.11%\n",
      "\t\t\t\t\t\t\t\t\tAverage % error = 33.16%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create 2nd degree polynomial feature set and train model\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "Xpoly_train = poly2.fit_transform(X_train)\n",
    "Xpoly_test = poly2.transform(X_test)\n",
    "print(f'Number of polynomial features: {Xpoly_train.shape[1]}')\n",
    "\n",
    "# Train our model\n",
    "reg.fit(Xpoly_train, y_train)\n",
    "ypoly_train_pred = reg.predict(Xpoly_train)\n",
    "ypoly_test_pred = reg.predict(Xpoly_test)\n",
    "\n",
    "get_error_scores(y_train, ypoly_train_pred, y_test, ypoly_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of polynomial features: 7770\n",
      "R SQUARED\n",
      "\tTrain R2:\t1.0\n",
      "\tTest R2:\t0.9693\n",
      "MEAN AVERAGE ERROR\n",
      "\tTrain MAE:\t0.0\n",
      "\tTest MAE:\t4554.24\n",
      "ROOT MEAN SQUARED ERROR\n",
      "\tTrain RMSE:\t0.0\n",
      "\tTest RMSE:\t32094.95\n",
      "\n",
      "10 Randomly selected results.\n",
      "Index: 583 \t- \tPrediction: $276,000 \tActual: $276,000 \tDifference: 0, 0.0%\n",
      "Index: 927 \t- \tPrediction: $162,000 \tActual: $162,000 \tDifference: 0, 0.0%\n",
      "Index: 724 \t- \tPrediction: $415,000 \tActual: $415,000 \tDifference: 0, 0.0%\n",
      "Index: 1195 \t- \tPrediction: $550,000 \tActual: $550,000 \tDifference: 0, 0.0%\n",
      "Index: 9 \t- \tPrediction: $379,000 \tActual: $379,000 \tDifference: 0, 0.0%\n",
      "Index: 32 \t- \tPrediction: $515,000 \tActual: $515,000 \tDifference: 0, 0.0%\n",
      "Index: 97 \t- \tPrediction: $260,000 \tActual: $260,000 \tDifference: 0, 0.0%\n",
      "Index: 1345 \t- \tPrediction: $559,900 \tActual: $559,900 \tDifference: 0, 0.0%\n",
      "Index: 1127 \t- \tPrediction: $100,000 \tActual: $100,000 \tDifference: 0, 0.0%\n",
      "Index: 80 \t- \tPrediction: $371,500 \tActual: $371,500 \tDifference: 0, 0.0%\n",
      "\t\t\t\t\t\t\t\t\tAverage % error = 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Create polynomial feature set and train model\n",
    "poly2 = PolynomialFeatures(degree=3)\n",
    "Xpoly_train = poly2.fit_transform(X_train)\n",
    "Xpoly_test = poly2.transform(X_test)\n",
    "print(f'Number of polynomial features: {Xpoly_train.shape[1]}')\n",
    "\n",
    "# Train our model\n",
    "reg.fit(Xpoly_train, y_train)\n",
    "ypoly_train_pred = reg.predict(Xpoly_train)\n",
    "ypoly_test_pred = reg.predict(Xpoly_test)\n",
    "\n",
    "# Check performance on train and test set\n",
    "get_error_scores(y_train, ypoly_train_pred, y_test, ypoly_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_10_15_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from lightgbm) (2.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from lightgbm) (1.15.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_10_15_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find best model\n",
    "def find_best_model (iX_train, iX_test, iy_train, iy_test):\n",
    "    #import needed modules\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from xgboost import XGBRegressor\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    #List models to discover\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(),\n",
    "        \"Random Forest\": RandomForestRegressor(),\n",
    "        \"XGBoost\": XGBRegressor(),\n",
    "        \"LightGBM\": LGBMRegressor(),\n",
    "        \"Neural Network\": MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500)\n",
    "    }\n",
    "\n",
    "    #Empty results dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for name, model in models.items():\n",
    "        print(f\"Processing {name}...\")\n",
    "        model.fit(iX_train, iy_train)\n",
    "        iy_pred = model.predict(iX_test)\n",
    "        mse = mean_squared_error(iy_test, iy_pred)\n",
    "        r2 = r2_score(iy_test, iy_pred)\n",
    "        \n",
    "        results[name] = {\"MSE\": mse, \"R² Score\": r2}\n",
    "\n",
    "    # Convert results to DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df_sorted = results_df.sort_values(by='R² Score', ascending=False)\n",
    "\n",
    "    print(f\"Processing COMPLETE\")\n",
    "\n",
    "    return results_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Linear Regression...\n",
      "Processing Decision Tree...\n",
      "Processing Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XGBoost...\n",
      "Processing LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 3381, number of used features: 31\n",
      "[LightGBM] [Info] Start training from score 344341.797693\n",
      "Processing Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1650: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_finding_results = find_best_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R² Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>1.372788e+09</td>\n",
       "      <td>0.959128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>2.155692e+09</td>\n",
       "      <td>0.935819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>2.608817e+09</td>\n",
       "      <td>0.922328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>5.537622e+09</td>\n",
       "      <td>0.835129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>9.167652e+09</td>\n",
       "      <td>0.727053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>9.933098e+09</td>\n",
       "      <td>0.704263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            MSE  R² Score\n",
       "XGBoost            1.372788e+09  0.959128\n",
       "LightGBM           2.155692e+09  0.935819\n",
       "Random Forest      2.608817e+09  0.922328\n",
       "Decision Tree      5.537622e+09  0.835129\n",
       "Linear Regression  9.167652e+09  0.727053\n",
       "Neural Network     9.933098e+09  0.704263"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finding_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env",
   "language": "python",
   "name": "lhl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
